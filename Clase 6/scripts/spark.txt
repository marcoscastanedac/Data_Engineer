Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.2.0
      /_/

Using Python version 3.8.10 (default, Mar 15 2022 12:22:08)
Spark context Web UI available at http://b7428173481e:4040
Spark context available as 'sc' (master = yarn, app id = application_1723302793829_0001).
SparkSession available as 'spark'.
>>> df = spark.read.csv("/ingest/yellow_tripdata_2021-01.csv")
>>> df.printSchema()
root
 |-- _c0: string (nullable = true)
 |-- _c1: string (nullable = true)
 |-- _c2: string (nullable = true)
 |-- _c3: string (nullable = true)
 |-- _c4: string (nullable = true)
 |-- _c5: string (nullable = true)
 |-- _c6: string (nullable = true)
 |-- _c7: string (nullable = true)
 |-- _c8: string (nullable = true)
 |-- _c9: string (nullable = true)
 |-- _c10: string (nullable = true)
 |-- _c11: string (nullable = true)
 |-- _c12: string (nullable = true)
 |-- _c13: string (nullable = true)
 |-- _c14: string (nullable = true)
 |-- _c15: string (nullable = true)
 |-- _c16: string (nullable = true)
 |-- _c17: string (nullable = true)

>>> df = spark.read.option("header", "true").csv("/ingest/yellow_tripdata_2021-01.csv")
>>> df.printSchema()
root
 |-- VendorID: string (nullable = true)
 |-- tpep_pickup_datetime: string (nullable = true)
 |-- tpep_dropoff_datetime: string (nullable = true)
 |-- passenger_count: string (nullable = true)
 |-- trip_distance: string (nullable = true)
 |-- RatecodeID: string (nullable = true)
 |-- store_and_fwd_flag: string (nullable = true)
 |-- PULocationID: string (nullable = true)
 |-- DOLocationID: string (nullable = true)
 |-- payment_type: string (nullable = true)
 |-- fare_amount: string (nullable = true)
 |-- extra: string (nullable = true)
 |-- mta_tax: string (nullable = true)
 |-- tip_amount: string (nullable = true)
 |-- tolls_amount: string (nullable = true)
 |-- improvement_surcharge: string (nullable = true)
 |-- total_amount: string (nullable = true)
 |-- congestion_surcharge: string (nullable = true)

>>> df.show(3)
+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+
|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|
+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+
|       1| 2021-01-01 00:30:10|  2021-01-01 00:36:12|              1|         2.10|         1|                 N|         142|          43|           2|          8|    3|    0.5|         0|           0|                  0.3|        11.8|                 2.5|
|       1| 2021-01-01 00:51:20|  2021-01-01 00:52:19|              1|          .20|         1|                 N|         238|         151|           2|          3|  0.5|    0.5|         0|           0|                  0.3|         4.3|                   0|
|       1| 2021-01-01 00:43:30|  2021-01-01 01:11:06|              1|        14.70|         1|                 N|         132|         165|           1|         42|  0.5|    0.5|      8.65|           0|                  0.3|       51.95|                   0|
+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+
only showing top 3 rows

>>> df.createOrReplaceTempView("v_df")
>>> df_cast = spark.sql("select cast(tpep_pickup_datetime as date), cast(passenger_count as int), cast(trip_distance as float) from v_df")
>>> df_cast.show(5)
+--------------------+---------------+-------------+
|tpep_pickup_datetime|passenger_count|trip_distance|
+--------------------+---------------+-------------+
|          2021-01-01|              1|          2.1|
|          2021-01-01|              1|          0.2|
|          2021-01-01|              1|         14.7|
|          2021-01-01|              0|         10.6|
|          2021-01-01|              1|         4.94|
+--------------------+---------------+-------------+
only showing top 5 rows

>>> df_cast.printSchema()
root
 |-- tpep_pickup_datetime: date (nullable = true)
 |-- passenger_count: integer (nullable = true)
 |-- trip_distance: float (nullable = true)

>>> df_cast.createOrReplaceTempView("v_cast")
>>> df_filter = spark.sql("select tpep_pickup_datetime, passenger_count, trip_distance from v_cast where passenger_count > 1 and trip_distance > 10")
>>> df_filter.show(5)
+--------------------+---------------+-------------+
|tpep_pickup_datetime|passenger_count|trip_distance|
+--------------------+---------------+-------------+
|          2021-01-01|              2|         19.1|
|          2021-01-01|              3|        10.74|
|          2021-01-01|              3|        16.54|
|          2021-01-01|              2|         11.1|
|          2021-01-01|              2|        15.19|
+--------------------+---------------+-------------+
only showing top 5 rows

>>> df_filter.createOrReplaceTempView("v_load")
>>> spark.sql("insert into tripsdb.trips select * from v_load")
2024-08-12 20:30:51,882 WARN conf.HiveConf: HiveConf of name hive.metastore.local does not exist
2024-08-12 20:30:53,305 WARN session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
DataFrame[]
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> df = spark.read.option("header", "true").csv("ingest/yellow_tripdata_2021.csv")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/hadoop/spark/python/pyspark/sql/readwriter.py", line 410, in csv
    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))
  File "/home/hadoop/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py", line 1309, in __call__
  File "/home/hadoop/spark/python/pyspark/sql/utils.py", line 117, in deco
    raise converted from None
pyspark.sql.utils.AnalysisException: Path does not exist: hdfs://172.17.0.2:9000/user/hadoop/ingest/yellow_tripdata_2021.csv
>>> df = spark.read.option("header", "true").csv("/ingest/yellow_tripdata_2021-01.csv")
>>> df_cast = df.select(df.tpep_pickup_datetime.cast("date"), df.passenger_count.cast("int"), df.trip_distance.cast("float"))
>>> df_cast.show(5)
+--------------------+---------------+-------------+
|tpep_pickup_datetime|passenger_count|trip_distance|
+--------------------+---------------+-------------+
|          2021-01-01|              1|          2.1|
|          2021-01-01|              1|          0.2|
|          2021-01-01|              1|         14.7|
|          2021-01-01|              0|         10.6|
|          2021-01-01|              1|         4.94|
+--------------------+---------------+-------------+
only showing top 5 rows

>>> df_cast.printSchema()
root
 |-- tpep_pickup_datetime: date (nullable = true)
 |-- passenger_count: integer (nullable = true)
 |-- trip_distance: float (nullable = true)

>>> df_filter = df_cast.filter( (df_cast.passenger_count > 1) & (df_cast.trip_distance > 10))
>>> df_filter.show(6)
+--------------------+---------------+-------------+
|tpep_pickup_datetime|passenger_count|trip_distance|
+--------------------+---------------+-------------+
|          2021-01-01|              2|         19.1|
|          2021-01-01|              3|        10.74|
|          2021-01-01|              3|        16.54|
|          2021-01-01|              2|         11.1|
|          2021-01-01|              2|        15.19|
|          2021-01-01|              2|         15.4|
+--------------------+---------------+-------------+
only showing top 6 rows

>>> df_filter.write.insertInto("tripsdb.trips")